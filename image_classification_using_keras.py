# -*- coding: utf-8 -*-
"""Image Classification using Keras

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/19gS9ZRs_yVUeBgH4OPe2v9IHm0ASf9If
"""

import numpy as np # Importing the Numpy module for matrix operations
import matplotlib.pyplot as plt # Importing matplotlib to display our images
import random
import os
import PIL
from keras import utils  # For one-hot encoding
from keras.models import Sequential # Type of model
from keras.layers import Dense, Dropout, Activation # Type of layers
from skimage.transform import resize # For scaling and resizing image arrays
from sklearn.metrics import confusion_matrix # To help make a confusion matrix

DATASET_PATH = "/content/dataset" # Dataset location

# Target resolution of the images
TARGET_WIDTH = 28
TARGET_HEIGHT = 28

INVERT = False # Inverting the image (white pixels become black and vice versa) because dark backgrounds can improve accuracy

VAL_RATIO = 0.2 # 20% of data set for validation
TEST_RATIO = 0.2 # 20% of data set for testing

random.seed(42)

### Load images as Numpy arrays

# We want to record the labels and assign a ground truth label as a number to each sample
labels = []
y_all = []    # All the labels to the dataset's images, 1-dimensional vector
X_all = []    # All images making up the dataset, 3-dimensional array: number of images x width x height

# Finding the directories in the dataset folder
for label in os.listdir(DATASET_PATH):
  class_dir = os.path.join(DATASET_PATH, label)
  if os.path.isdir(class_dir) and label != ".ipynb_checkpoints":

    # Adding the name of the folder to the labels list
    labels.append(label)

    # Going through each image in the folder
    for i, file in enumerate(os.listdir(class_dir)):

      # Skip the Jupyter Notebook checkpoints folder that sometimes gets added
      if file != ".ipynb_checkpoints":

        # Open each image and convert it to a grayscale version
        file_path = os.path.join(class_dir, file)
        img = PIL.Image.open(file_path).convert('L') # Converting to grayscale here

        # Converting each image to a numpy array
        img_array = np.asarray(img)
        if INVERT: # If we choose to invert, which we set invert=false so we will not invert
          img_array = 255 - img_array
        X_all.append(img_array)

        # Add each label to the labels array
        y_all.append(label)

    # Display how many images are loaded
    print("Added", str(i + 1), "images from", label)

num_samples = len(X_all) # Calculating the total number of samples

labels = sorted(labels) # Sorting the labels list by alphabetical order

# Printing out labels and number of samples
print(labels)
print("Number of samples:", num_samples)

print("Before:", y_all) # Displaying the labels before converting them to tehir indexes

# Converting each label to its index in the labels
y_out = []
for i, label in enumerate(y_all):
  y_out.append(labels.index(label))
y_all = y_out

print("After:", y_all) # Displaying the labels after the conversion

# Shuffling samples and associated labels to ensue the training set is useful
X_y = list(zip(X_all, y_all)) # Zipping each image to its corresponding label so they aren't shuffled from each other
random.shuffle(X_y) # Shuffling
X_all, y_all = zip(*X_y)

num_samples_test = int(TEST_RATIO * num_samples) # Calcualting the number of test samples
num_samples_val = int(VAL_RATIO * num_samples) # Calculating the number of validation samples

X_test = X_all[:num_samples_test] # The first however large the number of test samples is from the shuffled list's images is now the test set
y_test = y_all[:num_samples_test] # The first however large the number of test samples is from the shuffled list' labels is now the test set

X_val = X_all[num_samples_test:(num_samples_test + num_samples_val)]  # The first however large the number of validation samples is from the shuffled list's images is now the test set
y_val = y_all[num_samples_test:(num_samples_test + num_samples_val)] # The first however large the number of validation samples is from the shuffled list' labels is now the test set

X_train = X_all[(num_samples_test + num_samples_val):] # Remaining samples belong to the training set
y_train = y_all[(num_samples_test + num_samples_val):]

num_samples_train = len(X_train) # Number of samples in the training set

# Displaying the number of test, validation, and training samples
print("Number of test samples:", num_samples_test)
print("Number of validation samples:", num_samples_val)
print("Number of training samples:", num_samples_train)

idx = 0 # Index of the sample from the training set we want to view as an example

print("Label: " + str(y_train[idx]) + " (" + labels[y_train[idx]] + ")") # Printing out the label's index and string name
print(X_train[idx]) # Displaying the array of that particular image

plt.imshow(X_train[idx], cmap='gray', vmin=0, vmax=255) # Displaying the image

def resize_images(images, width, height, anti_aliasing=True):
  """
  Prove a list of Numpy arrays (in images parameter) to have them all resized to desired height and
  width. Returns the list of newly resized image arrays.

  NOTE: skimage resize returns *normalized* image arrays (values between 0..1)
  """
  X_out = []
  for i, img in enumerate(images):
    X_out.append(resize(img, (height, width), anti_aliasing=anti_aliasing))
  return X_out

X_train = resize_images(X_train, TARGET_WIDTH, TARGET_HEIGHT) # Resizing all images in the training set

X_val = resize_images(X_val, TARGET_WIDTH, TARGET_HEIGHT) # Resizing all images in the validation set

X_test = resize_images(X_test, TARGET_WIDTH, TARGET_HEIGHT) # Resizing all images in the test set

# Dispalying again after re-scale
idx = 0

print("Label: " + str(y_train[idx]) + " (" + labels[y_train[idx]] + ")")
print("First row:", X_train[idx][:1,:])

plt.imshow(X_train[idx], cmap='gray', vmin=0, vmax=1)

# Converting the training set into a Numpy array
X_train = np.asarray(X_train)
y_train = np.asarray(y_train)

# Converting validation set into a Numpy array
X_val = np.asarray(X_val)
y_val = np.asarray(y_val)

# Converting test set into a Numpy array
X_test = np.asarray(X_test)
y_test = np.asarray(y_test)

# Print out the Numpy array shapes
print("Training X:", X_train.shape)
print("Training y:", y_train.shape)
print("Validation X:", X_val.shape)
print("Validation y:", y_val.shape)
print("Test X:", X_test.shape)
print("Test y:", y_test.shape)

len_vector = TARGET_WIDTH * TARGET_WIDTH # length of the 1D vector we will flatten the images into

# Flattening each of the matrices
X_train = X_train.reshape(num_samples_train, len_vector)
X_val = X_val.reshape(num_samples_val, len_vector)
X_test = X_test.reshape(num_samples_test, len_vector)

# Determine the input shape for our Keras model (must be tuple)
input_shape = (X_train.shape[1],)

# Printing out shapes
print("X train:", X_train.shape)
print("y train:", y_train.shape)
print("X val:", X_val.shape)
print("y val:", y_val.shape)
print("X test:", X_test.shape)
print("y test:", y_test.shape)
print("Input tensor shape:", input_shape)

num_classes = len(labels) # Number of classes

# Using Keras's np_utils to create one-hot encoding
Y_train = utils.to_categorical(y_train, num_classes)
Y_val = utils.to_categorical(y_val, num_classes)
Y_test = utils.to_categorical(y_test, num_classes)

print("Y train:", Y_train.shape)
print("Y val:", Y_val.shape)
print("Y test:", Y_test.shape)

# Printing out a few examples from training set
# The one hot encoding takes our indexes for our labels: 0=background, 1=cap, 2=diode, 3=led, 4=resistor and outputs the probaibily that the label is part of each class.
for i in range(10):
  print("Label: " + str(y_train[i]) + " | One-hot:", Y_train[i])

# Sequential model is a linear stack of layers
model = Sequential()

# First layer
model.add(Dense(64, input_shape=input_shape)) # Fully connected, dense, layer
model.add(Activation('relu')) # RelU activation function
model.add(Dropout(0.25))

# Second layer
model.add(Dense(64)) # Input shape comes from previous layer
model.add(Activation('relu')) # RelU activation function
model.add(Dropout(0.25))

# Third layer
model.add(Dense(num_classes)) # 5 nodes, one for each class
model.add(Activation('softmax')) # Softmax activation function

model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc']) # Not a binary classifier so we use categorical crossentropy, we are also recording the accuracy metrics and using the adam optimizer

# Printing out model summary
print(model.summary())

# Training the model
history = model.fit(X_train,
                    Y_train,
                    batch_size=32, # batch size: 32 images per batch
                    epochs=200,
                    verbose=1,
                    validation_data=(X_val, Y_val))

# Extracting accuracy and loss values from the history
acc = history.history['acc']
val_acc = history.history['val_acc']
loss = history.history['loss']
val_loss = history.history['val_loss']

# Creating a list of epoch numbers
epochs = range(1, len(acc) + 1)

# Ploting training and validation loss values
plt.figure()
plt.plot(epochs, loss, color='blue', marker='.', label='Training loss')
plt.plot(epochs, val_loss, color='red', marker='.', label='Validation loss')
plt.title('Training vs Validation loss')
plt.legend()

# Ploting training and validation accuracies
plt.figure()
plt.plot(epochs, acc, color='blue', marker='.', label='Training accuracy')
plt.plot(epochs, val_acc, color='red', marker='.', label='Validation accuracy')
plt.title('Training vs Validation accuracy')
plt.legend()
plt.show()

idx = 3 # Index of a sample from the test set

x = np.expand_dims(X_test[idx], 0) # Making the sample a 2D array instead of a 1D vector as required by keras

y_pred = model.predict(x) # Making a prediction

# Finding the index of highest score from the one-hot encoding
predicted_label = np.argmax(y_pred)
actual_label = np.argmax(Y_val[idx])

# Displaying the model output, predicted label, and actual label
print("Model output:", y_pred)
print("Predicted label:", predicted_label, "-", labels[predicted_label])
print("Actual label:", actual_label, "-", labels[actual_label])

# Finding predictions from all test samples
Y_pred = model.predict(X_test)
print("Validation output shape:", Y_pred.shape)

# Converting the actual and predicted testing one-hot encoding to numerical labels
y_test = np.argmax(Y_test, axis=1)
y_pred = np.argmax(Y_pred, axis=1)

# Printing the first 50 values from actual and predicted testing sets
print("Actual test labels:\t", y_test[:50])
print("Predicted test labels:\t", y_pred[:50])

# Computing the confusion matrix
cm = confusion_matrix(y_test, y_pred)
cm = np.transpose(cm) # Transposing SKLearn matrix to make it match Edge Impulse

# Printing the confusion matrix
print()
print(" ---> Predicted labels")
print("|")
print("v Actual labels")
print("\t\t\t" + ' '.join("{!s:6}".format('(' + str(i) + ')') for i in range(num_classes)))
for row in range(num_classes):
  print("{:>12} ({}):  [{}]".format(labels[row], row, ' '.join("{:6}".format(i) for i in cm[row])))

score = model.evaluate(X_val, Y_val) # Model.evaluate gives an accuracy value and loss value on the validation set
print("Validation loss:", score[0]) # Validation set loss
print("Validation accuracy:", score[1]) # Validation set accuracy

score = model.evaluate(X_test, Y_test) # Gives the same accuracy and loss calculations on the test set
print("Test loss:", score[0]) # Test set loss
print("Test accuracy:", score[1]) # Test set accuracy